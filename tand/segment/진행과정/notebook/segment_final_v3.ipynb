{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwSclnWDfISH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Vis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from statistics import mode, StatisticsError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter \n",
    "from typing import List\n",
    "from tsfresh.feature_extraction import extract_features, ComprehensiveFCParameters\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "# ETC\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: \"{0:0.2f}\".format(x)}) # 소수점 출력 옵션 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5MX0cU9fISN"
   },
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnHZ-Kf2fISO"
   },
   "outputs": [],
   "source": [
    "def read_files(today, input_path, dates, platform_total = False):\n",
    "    input_path_list = [input_path]\n",
    "    if platform_total == True :\n",
    "        input_path_list = []\n",
    "        input_path_list.append(input_path.split('/')[0] +'/'+input_path.split('/')[1]+'/'+input_path.split('/')[2]+'/android')\n",
    "        input_path_list.append(input_path.split('/')[0] +'/'+input_path.split('/')[1]+'/'+input_path.split('/')[2]+'/ios')  \n",
    "    df_list = []\n",
    "    for input_path in input_path_list :\n",
    "        for i in range(1, dates+1):\n",
    "            try : \n",
    "                date = today - timedelta(days=i) # 설정한 today를 기준으로 과거 n일 json\n",
    "                y = str(date.year)[2:]\n",
    "                m = str(date.month).zfill(2)\n",
    "                d = str(date.day).zfill(2)\n",
    "                filename = y+m+d+'.json'\n",
    "                df_tmp = pd.read_json(input_path+'/'+filename)\n",
    "                df_list.append(df_tmp)\n",
    "            except ValueError :\n",
    "                print(\"Not enough data to load.\")\n",
    "    df = pd.concat(df_list).reset_index(drop = True) # data merge\n",
    "    return df\n",
    "\n",
    "def set_date_range(start, end, df):\n",
    "    start_point = str(start.year)+'-'+str(start.month)+'-'+str(start.day)\n",
    "    end_point = str(end.year)+'-'+str(end.month)+'-'+str(end.day)\n",
    "    \n",
    "    start_point = pd.Timestamp(start_point)\n",
    "    end_point = pd.Timestamp(end_point)\n",
    "    \n",
    "    # real_datae : 위에서 만든 date값중 오류가 있는 날짜 존재\n",
    "    # events : List안에 Dict형태로 데이터가 들어가 있음\n",
    "    df['real_date'] = df['events'].apply(lambda x: x[0]['date'])\n",
    "    df['real_date'] = pd.to_datetime(df['real_date'])\n",
    "\n",
    "    # 지정된 start와 end 사이의 날짜만 선택\n",
    "    selected_df = df.loc[df['real_date'].apply(lambda x: start_point <= x)]\n",
    "    selected_df = selected_df.loc[selected_df['real_date'].apply(lambda x: x < end_point)]\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIsqe0ZKfISO"
   },
   "source": [
    "# 2. Basic preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia6mDYu1fISP"
   },
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "def basic_preprocess(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['duration'] = round(df['duration']/1000)\n",
    "\n",
    "    #df = make_end_date(df)\n",
    "    #df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "\n",
    "    df_1 = real_date(df)\n",
    "    df_1 = numeric_anomaly_detection(df_1, 'duration')\n",
    "    df_1 = duration_outlier_detection(df_1) # 임시로 duration outlier 비활성화\n",
    "    df_1 = df_1.sort_values('date')\n",
    "    return df_1\n",
    "\n",
    "def real_date(df) :\n",
    "    # real_date(date변수에 오류 존재하여 event date대로 대체)\n",
    "    df['real_date'] = df['events'].apply(lambda x: x[0]['date'])\n",
    "    df['real_date'] = pd.to_datetime(df['real_date'])\n",
    "    \n",
    "    # 기존 date에는 real_date를 기준으로 날짜만 남기기\n",
    "    df['date'] = df['real_date'].apply(lambda x : x.date())\n",
    "    df['date_ymd'] = df['real_date']\n",
    "    return df\n",
    "\n",
    "def make_end_date(df):\n",
    "    df['end_date'] = df['date'] + df['duration'].apply(lambda x: timedelta(seconds=int(x)))\n",
    "    return df\n",
    "\n",
    "def duration_outlier_detection(df):\n",
    "    # skew가 클 경우 상위 90% 미만의 데이터만 선택\n",
    "    if (df['duration'].skew()<-1) or (df['duration'].skew()>1):\n",
    "        Q1 = df['duration'].quantile(0.999)\n",
    "        filter = df['duration'] < Q1\n",
    "        df=df.loc[filter]\n",
    "    else:\n",
    "        Q1 = df['duration'].quantile(0.25)\n",
    "        Q3 = df['duration'].quantile(0.75)\n",
    "        IQR = Q3 - Q1 #IQR : InterQuartileRange\n",
    "        filter = (df['duration'] >= Q1 - 1.5 * IQR) & (df['duration'] <= Q3 + 1.5 *IQR)\n",
    "        df=df.loc[filter]\n",
    "        \n",
    "    df=df.loc[df['duration']>0]\n",
    "    return df\n",
    "\n",
    "def find_datetime(x):\n",
    "    if type(x) == pd.Timestamp:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_digit(x):\n",
    "    try:\n",
    "        tmp = float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def date_anomaly_detection(df,date_cols):\n",
    "    if isinstance(date_cols,str):\n",
    "        df = df.drop(df.loc[df[date_cols].apply(lambda x: find_datetime(x))==False].index,axis=0)\n",
    "    elif isinstance(date_cols,list):\n",
    "        select  = df[date_cols].apply(lambda r : all([find_datetime(e) for e in r  ]),axis=1) \n",
    "        df=df[select]\n",
    "    else:\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "def numeric_anomaly_detection(df,num_cols):\n",
    "    if isinstance(num_cols,str):\n",
    "        select  = df[num_cols].apply(lambda r : is_digit(r)) \n",
    "        df=df[select]\n",
    "    elif isinstance(num_cols,list):\n",
    "        select  = df[num_cols].apply(lambda r : all([is_digit(e) for e in r  ]),axis=1) \n",
    "        df=df[select]\n",
    "    else:\n",
    "        pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_1oER_yfISP"
   },
   "source": [
    "# 3. Select Target\n",
    "### (New-visitors vs Re-visitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhZGKTGSfISQ"
   },
   "outputs": [],
   "source": [
    "# 1) divide time range\n",
    "## (1) make date to real date\n",
    "def real_date(df) :\n",
    "    # real_date(date변수에 오류 존재하여 event date대로 대체)\n",
    "    df['real_date'] = df['events'].apply(lambda x: x[0]['date'])\n",
    "    df['real_date'] = pd.to_datetime(df['real_date'])\n",
    "    \n",
    "    # 기존 date에는 real_date를 기준으로 날짜만 남기기\n",
    "    df['date'] = df['real_date'].apply(lambda x : x.date())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnIX0TYSfISQ"
   },
   "outputs": [],
   "source": [
    "def abstract_events(x):\n",
    "    \"\"\"get events name from list\"\"\"\n",
    "    path = []\n",
    "    for i in x:\n",
    "        path.append(i['name'])\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0JNDwgofISR"
   },
   "outputs": [],
   "source": [
    "def divide_visitor(today, df, key_id, newb_period = 14, user_property = False) :\n",
    "    \"\"\"divide new visitor and re-visitors\"\"\"\n",
    "    # 1) set data range to extract new visitor\n",
    "    df_last = set_date_range(today-timedelta(newb_period), today, df)\n",
    "\n",
    "    # 2) abstract events\n",
    "    df_last['abs_events'] = df_last.loc[:, 'events'].apply(lambda x : abstract_events(x))\n",
    "\n",
    "    # 3) get new_visitors id\n",
    "    if user_property == False :\n",
    "        standard_event = '#appInstall'\n",
    "        new_idx = df_last['abs_events'].apply(lambda x : True if standard_event in x else False)\n",
    "\n",
    "    else :\n",
    "        standard_event = 'signUp'\n",
    "        new_idx = df_last['abs_events'].apply(lambda x : True if standard_event in x else False)\n",
    "\n",
    "    # 4) divide new and re visitor\n",
    "    df_new_visitor_id = pd.DataFrame({key_id : list(set(df_last_2w[new_idx][key_id]))})\n",
    "    df_new = pd.merge(df_last, df_new_visitor_id,\n",
    "                      on = key_id, how = 'right')\n",
    "    df_re = pd.merge(df, df_new_visitor_id,\n",
    "                     on = key_id, how = 'outer', indicator= True)\\\n",
    "                    .query('_merge != \"both\"').drop(['_merge'], 1)\n",
    "\n",
    "    return df_new, df_re\n",
    "\n",
    "\n",
    "def divide_visitor_v2(today, dates, df, key_id, newb_period = 7, user_property = False) :\n",
    "    \"\"\"divide new visitor and re-visitors\"\"\"\n",
    "    # 1) set data range to extract new visitor\n",
    "    df_last = set_date_range(today-timedelta(dates),\n",
    "                             today-timedelta(dates)+timedelta(newb_period),\n",
    "                             df)\n",
    "\n",
    "    # 2) abstract events\n",
    "    df_last['abs_events'] = df_last.loc[:, 'events'].apply(lambda x : abstract_events(x))\n",
    "    df['abs_events'] = df.loc[:, 'events'].apply(lambda x : abstract_events(x))\n",
    "\n",
    "    # 3) get new_visitors id\n",
    "    if user_property == False :\n",
    "        standard_event = '#appInstall'\n",
    "        new_idx = df_last['abs_events'].apply(lambda x : True if standard_event in x else False)\n",
    "        delete_new_idx = df['abs_events'].apply(lambda x : True if standard_event in x else False) \n",
    "\n",
    "    else :\n",
    "        standard_event = 'signUp'\n",
    "        new_idx = df_last['abs_events'].apply(lambda x : True if standard_event in x else False)\n",
    "        delete_new_idx = df['abs_events'].apply(lambda x : True if standard_event in x else False) \n",
    "\n",
    "        \n",
    "    # 4) divide new and re visitor\n",
    "    ## (1) df_new\n",
    "    df_new_visitor_id_target = pd.DataFrame({key_id : list(set(df_last[new_idx][key_id]))})\n",
    "    df_new = pd.merge(df, df_new_visitor_id_target,\n",
    "                      on = key_id, how = 'right')\n",
    "    ## (2) df_re\n",
    "    df_new_visitor_id_delete = pd.DataFrame({key_id : list(set(df[delete_new_idx][key_id]))})\n",
    "    df_re = pd.merge(df, df_new_visitor_id_delete,\n",
    "                     on = key_id, how = 'outer', indicator= True)\\\n",
    "                    .query('_merge != \"both\"').drop(['_merge'], 1)\n",
    "\n",
    "    return df_new, df_re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z8bWR48fISR"
   },
   "source": [
    "# 4. Activity_index\n",
    " : act_index = duration + visit_cnt + conti_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rOYXno_fISS"
   },
   "outputs": [],
   "source": [
    "def duration_mean(df) : \n",
    "    duration_mean = round(df['duration'].mean(), 2)\n",
    "    return duration_mean\n",
    "\n",
    "# def duration_sum(df) : \n",
    "#     duration_sum = df['duration'].sum()\n",
    "#     return duration_sum\n",
    "\n",
    "def act_days_last7(df, date) :\n",
    "    # last7\n",
    "#     last7 = df[df['date'].map(lambda x : x >= date.date())]['date']\n",
    "#     last7 = last7['date'].map(lambda x : x.day).unique()\n",
    "\n",
    "    # last7(v2) : only for 7days in data(don't neet to setting range)\n",
    "    last7 = df['date'].map(lambda x : x.day).unique()\n",
    "    return len(last7)\n",
    "\n",
    "def conti_act_days_last7(df, date): \n",
    "#     last7 = df[df['date'].map(lambda x: x>date)]['date'] \n",
    "#     last7 = last7.map(lambda x: x.day).unique().tolist() # series형태로 날짜가 들어있기에 map(lambda x : x.day)으로 하나씩 값을 꺼내와 날짜만 추출한 뒤, 유니크한 값을 list 형태로 반환\n",
    "    last7 = df['date'].map(lambda x : x.day).unique().tolist()\n",
    "    output = 0\n",
    "    if len(last7) == 0:\n",
    "        return output\n",
    "    elif len(last7) == 1:\n",
    "#         output = 1\n",
    "        output = 0 \n",
    "        return output\n",
    "    else:\n",
    "        conti = 1\n",
    "        for i in range(len(last7)-1):\n",
    "            if last7[i+1] - last7[i] == 1:\n",
    "                conti+=1\n",
    "                if conti > output:\n",
    "                    output = conti\n",
    "            else:\n",
    "                conti=1\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnR-Yl6kfISS"
   },
   "outputs": [],
   "source": [
    "# make weekly_visit_cnt & weekly_activity_index\n",
    "def weekly_visit_act (df) :\n",
    "    df['visit_week_cnt'] = df.apply(lambda x: 4-sum(x[1:5]==0) ,axis=1)\n",
    "    df['act_index_mean'] = round(df.iloc[:, 1:5].sum(axis = 1)/df.visit_week_cnt,3)\n",
    "    return df \n",
    "\n",
    "# act_index labeling [H/M/L]\n",
    "def act_index_labeling (df) :\n",
    "    \"\"\"\n",
    "    최종 활동지수 라벨링\n",
    "    H : 전체 상위 33% \n",
    "    M : 전체 중위 33%\n",
    "    L : 전체 하위 33%\n",
    "    \"\"\"\n",
    "    high_point = round((df['act_index_mean'].max() -df['act_index_mean'].min()) *0.66, 2) + df['act_index_mean'].min() \n",
    "    low_point = round((df['act_index_mean'].max() -df['act_index_mean'].min()) *0.33, 2) + df['act_index_mean'].min()\n",
    "    df['act_label'] = ''\n",
    "    df.loc[(df['visit_week_cnt'] >= 3) &\n",
    "       (df['act_index_mean'] > low_point),'act_label'] = 'H'\n",
    "    df.loc[(df['visit_week_cnt'] >= 3) &\n",
    "       (df['act_index_mean'] <= low_point),'act_label'] = 'M'\n",
    "    df.loc[(df['visit_week_cnt'] == 2) &\n",
    "       (df['act_index_mean'] > low_point),'act_label'] = 'M'\n",
    "    df.loc[(df['visit_week_cnt'] == 2) &\n",
    "       (df['act_index_mean'] <= low_point),'act_label'] = 'L'\n",
    "    df.loc[(df['visit_week_cnt'] == 1), 'act_label'] = 'L'\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PHOt5XwfISS"
   },
   "source": [
    "# 4. purchase_index\n",
    " : purchase_index = buy_cnt + buy_amt\n",
    "  - id별로 몇번을 구매했는지 컬럼, 총 얼마의 금액을 구매했는지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0605Fi5FfIST"
   },
   "outputs": [],
   "source": [
    "# json flatter\n",
    "def flatten_json(nested_json, exclude=['']):\n",
    "    \"\"\"Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "            exclude: Keys to exclude from output.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    def flatten(x, name='', exclude=exclude):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                if a not in exclude: flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out\n",
    "    \n",
    "def json_to_dataframe_nodeN(df,key):\n",
    "    df['event_json']=df.apply(lambda x: [flatten_json(j) for j in x['events']],axis=1)\n",
    "    l = df['event_json'].str.len()\n",
    "    event=pd.DataFrame(np.concatenate(np.array(df['event_json'])).tolist(), index=np.repeat(df[key], l))\n",
    "    event[key_id]=event.index.values\n",
    "    event.index=list(range(0,event.shape[0]))\n",
    "    return df, event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrrnFIc-fIST"
   },
   "outputs": [],
   "source": [
    "# df, event =  json_to_dataframe_nodeN(df,key_id)\n",
    "def commerce_preprocess(event, key_id, event_buy, event_refund,\n",
    "                        orderId_param = None, price_param = None,) :\n",
    "    \"\"\"\n",
    "    - coke has 2store(store and vending) ---(could be different by app)\n",
    "        - event_buy = ['sapBuyStore','sapBuyVending']\n",
    "        - event_refund = ['sapRefundStore','sapRefundVending']\n",
    "    \"\"\"\n",
    "    ## (1) buy / refund \n",
    "    df_event_buy = pd.DataFrame() \n",
    "    for i in event_buy:\n",
    "        df_tmp = event.loc[event['name'] == i,[key_id,orderId_param,price_param]]\n",
    "        df_event_buy = pd.concat([df_event_buy, df_tmp])\n",
    "\n",
    "    df_event_refund  = pd.DataFrame() \n",
    "    for i in event_refund :\n",
    "        df_tmp = event.loc[event['name'] == i,[key_id,orderId_param,price_param]]\n",
    "        df_event_refund = pd.concat([df_event_refund, df_tmp])\n",
    "    \n",
    "    ## (2) merge buy & refund\n",
    "    merge_tmp = pd.merge(df_event_buy, df_event_refund.rename(columns = {price_param : 'refund_price'}),\n",
    "                         on = [key_id, orderId_param], \n",
    "                         how = 'left')\n",
    "\n",
    "    ## (3) Exclude the refund\n",
    "    merge_tmp['refund_price'] = merge_tmp['refund_price'].fillna(0)\n",
    "    merge_tmp['priceFinal'] = merge_tmp[price_param] - merge_tmp['refund_price']\n",
    "    commerce_df = merge_tmp.loc[merge_tmp['priceFinal'] != 0, [key_id, orderId_param, 'priceFinal']]\n",
    "    \n",
    "    return commerce_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZBjJz0KfIST"
   },
   "outputs": [],
   "source": [
    "def commerce_feature (df, key_id) :\n",
    "    \"\"\"\n",
    "    make total Price and buyCount column\n",
    "    \"\"\"\n",
    "    # 1) PriceFinal by key_id\n",
    "    tmp_price = df.groupby(key_id).sum().reset_index()\n",
    "    # 2) buyCount by key_id\n",
    "    tmp_cnt = df.groupby(key_id).count().reset_index()\\\n",
    "            [[key_id, 'priceFinal']].rename(columns = {'priceFinal' : 'buyCnt'})\n",
    "    # 3) merge price and cnt\n",
    "    tmp_final = pd.merge(tmp_price, tmp_cnt, on = key_id)\n",
    "    # 4) priceFinal_mean & priceFinal_log\n",
    "    tmp_final['buyCnt_log'] = np.log(tmp_final['buyCnt']+1)\n",
    "    tmp_final['price_mean'] = tmp_final['priceFinal']/tmp_final['buyCnt']\n",
    "    tmp_final['price_Meanlog'] = np.log(tmp_final['price_mean'])\n",
    "\n",
    "    return tmp_final\n",
    "\n",
    "def commerce_index_labeling (commerce_final,  buyCnt_weight) :\n",
    "    \"\"\"\n",
    "    최종 구매지수 라벨링\n",
    "    H : 2회 구매자 평균 purchase_index이상 \n",
    "    M : 1회 평균 purchase_index이상 ~ 2회 평균 purchase_index이하 \n",
    "    L : 1회 평균 purchase_index이하 \n",
    "    0 : 0회 구매자\n",
    "    \"\"\"\n",
    "    # 1) min_max normalize\n",
    "    minMax_mat = commerce_final[['buyCnt_log', 'price_Meanlog']].to_numpy()\n",
    "    scaler = MinMaxScaler().fit(minMax_mat)\n",
    "    minMax_mat = scaler.transform(minMax_mat)\n",
    "    \n",
    "    # 2) make purchase_index using buyCnt_log + price_Meanlog\n",
    "    w_1 = buyCnt_weight \n",
    "    w_2 = 1 - w_1\n",
    "    # log buycnt & price\n",
    "    commerce_final['purchase_index'] = ((minMax_mat[:,0])*w_1 + minMax_mat[:,1]*w_2)\n",
    "    commerce_final['purchase_index'] = round(commerce_final['purchase_index'], 2)  \n",
    "    commerce_final.head()\n",
    "    \n",
    "    \n",
    "    # 3) labeling\n",
    "    high_point = commerce_final[commerce_final['buyCnt'] == 2].purchase_index.mean()\n",
    "    low_point = commerce_final[commerce_final['buyCnt'] == 1].purchase_index.mean()\n",
    "\n",
    "    commerce_final['purchase_label'] = ''\n",
    "    commerce_final.loc[commerce_final['purchase_index'] >  high_point, 'purchase_label'] = 'H' # 83\n",
    "    commerce_final.loc[(commerce_final['purchase_index'] <=  high_point) &\n",
    "                   (commerce_final['purchase_index'] >  low_point), 'purchase_label'] = 'M'# 227\n",
    "    commerce_final.loc[(commerce_final['purchase_index'] <=  low_point) &\n",
    "                   (commerce_final['purchase_index'] >  0), 'purchase_label'] = 'L'# 212\n",
    "    commerce_final.loc[(commerce_final['purchase_index'] ==0), 'purchase_label'] = 0 # 157,390\n",
    "    \n",
    "    return commerce_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeuT0gdJfISU"
   },
   "source": [
    "# 5-1. Activity_Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTh-UDvifISU"
   },
   "outputs": [],
   "source": [
    "def activity_agg(df, today, key_id, duration_weight = 0.5) :\n",
    "    groupes = df.groupby(key_id)\n",
    "    ids = list(set(df[key_id]))\n",
    "#     print(str(today)+'_number of unique customer : ' + str(len(ids)))\n",
    "\n",
    "    # summarise\n",
    "    listForDF = []\n",
    "    for i in ids :\n",
    "        tmp = groupes.get_group(i)\n",
    "        mp = {}\n",
    "        mp[key_id] = i\n",
    "        mp['period'] = today\n",
    "        # mp['start'] = mp['date'].apply(lambda x : x.date()).min()\n",
    "        # mp['end'] = mp['date'].apply(lambda x : x.date()).max()\n",
    "        mp['conti_act_days_last7'] = conti_act_days_last7(tmp, today)\n",
    "        mp['act_days_last7'] = act_days_last7(tmp, today)\n",
    "        mp['duration_mean_log'] = np.log(duration_mean(tmp))\n",
    "        mp['act_conti_sum'] = mp['act_days_last7'] + mp['conti_act_days_last7']*0.5\n",
    "        listForDF.append(mp)\n",
    "    df = pd.DataFrame(listForDF)\n",
    "\n",
    "    # act_index (min_max)\n",
    "    w_1 = duration_weight \n",
    "    w_2 = 1 - w_1\n",
    "    minMax_mat = df[['act_conti_sum', 'duration_mean_log']].to_numpy()\n",
    "    scaler = MinMaxScaler().fit(minMax_mat)\n",
    "    minMax_mat = scaler.transform(minMax_mat)\n",
    "    df['act_index'] = ((minMax_mat[:,0])*w_1 + minMax_mat[:,1]*w_2)\n",
    "    df['act_index'] = round(df['act_index'], 2) + 1 # +1 for ditinguish them from unvisited people\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMloYwkufISU"
   },
   "source": [
    "# 5-2. purchase_Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqi_bUtpfISU"
   },
   "outputs": [],
   "source": [
    "def purchase_agg(df, key_id, event_buy = None, event_refund = None, \n",
    "                 orderId_param = None, price_param = None) :\n",
    "    # 1) json flatten\n",
    "    df, event = json_to_dataframe_nodeN(df, key_id)\n",
    "    \n",
    "    # 2) parameter setting (incase different tagging name)\n",
    "    \"\"\"아래 파라미터값 확인 필요\"\"\"\n",
    "    if event_buy == None :\n",
    "        event_buy = ['sapBuyStore', 'sapBuyVending']\n",
    "    if event_refund == None :\n",
    "        event_refund = ['sapRefundStore', 'sapRefundVending']\n",
    "    if orderId_param == None :\n",
    "        orderId_param = 'params__sapOrderId'\n",
    "    if price_param == None :\n",
    "        price_param = 'params__sapPriceFinal'\n",
    "    \n",
    "    # 3) feature Engineer\n",
    "    commerce_tmp = commerce_preprocess(event, key_id,\n",
    "                                       event_buy,event_refund, \n",
    "                                       orderId_param , price_param)\n",
    "    commerce_tmp = commerce_feature(commerce_tmp, key_id) \n",
    "        \n",
    "    # 4) merge with non-buyer (df : total data)\n",
    "    df_commerce_merge = pd.merge(pd.DataFrame({key_id : list(set(df.sphereId))}),\n",
    "                                 commerce_tmp,\n",
    "                                 on = key_id, how = 'outer').fillna(0)\n",
    "    \n",
    "    # 5) purchase index(min_max) & labeling \n",
    "    buyCnt_weight = 0.8 # 구매횟수에 대한 가중치 수정시 변경\n",
    "    commerce_final = commerce_index_labeling(df_commerce_merge,  buyCnt_weight)\n",
    "\n",
    "    return commerce_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46GMaUpefISV"
   },
   "source": [
    "\n",
    "# 5. segment_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_1petMvfISV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# main\n",
    "def basic_segment_main(df, today, dates, key_id, commerce = False) :\n",
    "    # 1) activity \n",
    "    ## (1)divide time range\n",
    "    df_act_merge = pd.DataFrame()\n",
    "    for i in range(int(dates/7)) :\n",
    "        end_day = (today - timedelta(days = 7 * (int(dates/7) - (i + 1))+1)).date()\n",
    "        start_day = end_day - timedelta(days = 7)\n",
    "        df_tmp = df[(df['date'] <= end_day) & (df['date'] > start_day)] \n",
    "        ## (2) apply activity agg\n",
    "        df_tmp = activity_agg(df_tmp, end_day, key_id)[[key_id,'act_index']]\n",
    "        df_tmp = df_tmp.rename(columns = {'act_index' : str(end_day)})\n",
    "        ## (3) merge weekly activity index\n",
    "        try : \n",
    "            df_act_merge = pd.merge(df_act_merge, df_tmp,on = key_id, how = 'outer')\n",
    "        except KeyError :\n",
    "            df_act_merge = df_tmp\n",
    "            pass\n",
    "    df_act_merge = df_act_merge.fillna(0) # fill Na with 0\n",
    "    \n",
    "    ## (4) 'weekly_visit_cnt & weekly_act_mean' for act_index  \n",
    "    df_act_merge = weekly_visit_act(df_act_merge) \n",
    "    df_act_merge = act_index_labeling(df_act_merge)\n",
    "    df_act_final = df_act_merge[[key_id,'act_label']]\n",
    "    df_final = df_act_final\n",
    "    \n",
    "    # 2) phurchase_agg                \n",
    "    if commerce == True :\n",
    "        df_commerce_tmp = purchase_agg(df, key_id) \n",
    "        \n",
    "        # merge act & commerce\n",
    "        df_final = pd.merge(df_act_merge, df_commerce_tmp,\n",
    "                    on = key_id)\n",
    "        df_final[['act_label', 'purchase_label']]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGSNeKMBfISV"
   },
   "source": [
    "# 6. final_segments_divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW0dqAYLfISV"
   },
   "outputs": [],
   "source": [
    "# feature select\n",
    "def feature_select (df, key_id, commerce = False) : \n",
    "    if commerce == False :\n",
    "        df_re_segment_detail = df[[key_id,'visit_week_cnt', 'act_index_mean', \n",
    "                                             'act_label']]\n",
    "        df_re_segment_label = df[[key_id, 'act_label']]\n",
    "    else : \n",
    "        df_re_segment_detail = df[[key_id,'visit_week_cnt', 'act_index_mean', \n",
    "                                             'buyCnt','priceFinal', 'price_mean', 'purchase_index',\n",
    "                                             'act_label', 'purchase_label']]\n",
    "        df_re_segment_label = df[[key_id, 'act_label','purchase_label']]\n",
    "\n",
    "    return df_re_segment_detail, df_re_segment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47fdjYk5fISW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final segments divide \n",
    "def final_segments_divider(df, commerce = False) :\n",
    "    \"\"\"\n",
    "    (1) 비커머스 \n",
    "    act : H ==> Heavy\n",
    "    act : M ==> Light\n",
    "    (2) 커머스 \n",
    "    purchase : H ==> Heavy \n",
    "    purchase : M & act : H,M==> Heavy\n",
    "    purchase : L ==> Light\n",
    "    purchase : L & act : M ==> Light\n",
    "    \"\"\"\n",
    "    \n",
    "    df['segment'] = 'None'\n",
    "    \n",
    "    if commerce == False :\n",
    "        ## loyal\n",
    "        df.loc[df['act_label'] == 'H', 'segment'] = 'HEAVY'\n",
    "        ## light\n",
    "        df.loc[df['act_label'] == 'M', 'segment'] = 'LIGHT'\n",
    "    else :\n",
    "        # pruchase\n",
    "        ## loyal\n",
    "        df.loc[df['purchase_label'] == 'H', 'segment'] = 'HEAVY'\n",
    "        df.loc[(df['act_label'] == 'H') & (df['purchase_label'] == 'M'), 'segment'] = 'HEAVY' \n",
    "\n",
    "        ## light\n",
    "        df.loc[df['purchase_label'] == 'L', 'segment'] = 'LIGHT'\n",
    "        df.loc[(df['act_label'] == 'H') & (df['purchase_label'] == 0), 'segment'] = 'LIGHT' \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxrEdcmVfISW"
   },
   "source": [
    "# 7. growth index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkcPsn7RfISW"
   },
   "outputs": [],
   "source": [
    "def make_real_date(x,y):\n",
    "    real_date_list = []\n",
    "    for i in x:\n",
    "        if i in y.keys():\n",
    "            real_date_list.append(y[i]) # {날짜 : 시간} dict에 날짜 key값으로 시간 출력 \n",
    "        else:\n",
    "            real_date_list.append(0)\n",
    "    return real_date_list\n",
    "\n",
    "def conti_corr(x):\n",
    "    global idx\n",
    "    corr, p = pearsonr(idx, x)\n",
    "    return corr\n",
    "\n",
    "def min_engage_2w(x):\n",
    "    x_2w = [c for c in x[-14:] if c != 0]\n",
    "    return len(x_2w)\n",
    "\n",
    "def preprocess_potential_users(data):\n",
    "    global date_list\n",
    "    \n",
    "    poten_df = data[[key_id,'date','duration', 'real_date']]\n",
    "    poten_df['date'] = pd.to_datetime(poten_df['date'])\n",
    "    poten_df['date_time'] = pd.to_datetime(poten_df['real_date'])\n",
    "    poten_df['date'] = poten_df['date_time'].astype(str).apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    poten_df = poten_df.sort_values(by=[key_id,'date'],ascending=False)\n",
    "    poten_df['date_ymd'] = poten_df['date'].dt.date\n",
    "    poten_df = poten_df.groupby(['sphereId','date_ymd'])['duration'].agg('sum').reset_index()\n",
    "\n",
    "    tmp = poten_df.groupby(key_id)\n",
    "    poten_df_agg = pd.DataFrame()\n",
    "    poten_df_agg[key_id] = list(tmp.groups.keys())\n",
    "    poten_df_agg['date'] =  poten_df.groupby(key_id)['date_ymd'].apply(list).values\n",
    "    poten_df_agg['duration'] =  poten_df.groupby(key_id)['duration'].apply(list).values\n",
    "    poten_df_agg['real_date'] = [sorted(date_list) for i in range(poten_df_agg.shape[0])]\n",
    "    poten_df_agg['mapping'] = poten_df_agg.apply(lambda x: dict(zip(x['date'],x['duration'])), axis=1) # 시간과 날짜 꺼내와서 dict 형태로 묶기\n",
    "\n",
    "    poten_df_agg['real_duration'] = poten_df_agg.apply(lambda x: make_real_date(x['real_date'],x['mapping']), axis=1) # 접속한 날짜만 시간이 있고, 나머지 날짜는 0으로 가득 차있는 리스트\n",
    "    poten_df_agg['duration_corr'] = poten_df_agg['real_duration'].apply(lambda x: conti_corr(x)) # corr : 기울기\n",
    "    poten_df_agg['min_engage_2w'] = poten_df_agg.real_duration.apply(lambda x: min_engage_2w(x)) # 뒤 2주에 접속한 수\n",
    "    poten_df_agg = poten_df_agg.loc[(poten_df_agg['min_engage_2w']>=3)&(poten_df_agg['duration_corr']>=0.2)] # 뒤 2주에 3번이상 접속하였고, 기울기가 0.2 이상인 사용자만 선택\n",
    "    return poten_df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD66Aj0AfISW"
   },
   "source": [
    "# 99. run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-OeTFXvfISW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# today = datetime(2020,5,31)\n",
    "# input_path = '../in/coke/android'\n",
    "# key_id = 'sphereId'\n",
    "# dates = 28\n",
    "\n",
    "# newb_period = 7\n",
    "# user_property = False\n",
    "# commerce = True \n",
    "# platform_total = False\n",
    "\n",
    "\n",
    "# # 1) data import\n",
    "# df = read_files(today, input_path, dates, platform_total)\n",
    "\n",
    "# # 2) Basic preprocess\n",
    "# df = basic_preprocess(df) # duration outlier\n",
    "\n",
    "# #3) divide_target\n",
    "# # df_new, df_re = divide_visitor(today, df,key_id,\n",
    "# #                                newb_period=14,\n",
    "# #                                user_property = False) \n",
    "# df_new, df_re = divide_visitor_v2(today, dates, df,key_id,\n",
    "#                                   newb_period=7,\n",
    "#                                   user_property = False)\n",
    "\n",
    "# 4) basic_segment_main\n",
    "# df_re_index = basic_segment_main(df_re, today, dates,\n",
    "#                                    key_id, commerce)\n",
    "# # 5) feature select\n",
    "# df_re_index_detail, df_re_index = feature_select(df_re_index, \n",
    "#                                                  key_id, commerce)\n",
    "\n",
    "# # 6) final_segments_divider\n",
    "# df_re_segment = final_segments_divider(df_re_index, commerce=True)\n",
    "\n",
    "# df_re_growth = preprocess_potential_users(df_re)\n",
    "# df_new_growth = preprocess_potential_users(df_new)\n",
    "# #     return df_re_segment, df_re_growth, df_new_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5KcSxNafISX"
   },
   "outputs": [],
   "source": [
    "def segment_run (today, input_path, dates, key_id, newb_period = 14, \n",
    "                 user_property = False, commerce = True, \n",
    "                 platform_total = False) :\n",
    "    # 1) data import\n",
    "    df = read_files(today, input_path, dates, platform_total)\n",
    "\n",
    "    # 2) Basic preprocess\n",
    "    df = basic_preprocess(df) # duration outlier\n",
    "\n",
    "    # 3) divide_target\n",
    "#     df_new, df_re = divide_visitor(today, df,key_id,\n",
    "#                                    newb_period=14,\n",
    "#                                    user_property = False) \n",
    "    df_new, df_re = divide_visitor_v2(today, dates, df,key_id,\n",
    "                                  newb_period=7,\n",
    "                                  user_property = False)\n",
    "    \n",
    "    # 4) basic_segment_main\n",
    "    df_re_index = basic_segment_main(df_re, today, dates,\n",
    "                                       key_id, commerce)\n",
    "     \n",
    "    # 5) feature select\n",
    "    df_re_index_detail, df_re_index = feature_select(df_re_index, \n",
    "                                                     key_id, commerce)\n",
    "     \n",
    "    # 6) basic_segments_divider\n",
    "    df_re_segment = final_segments_divider(df_re_index, commerce=True)\n",
    "\n",
    "    # 7) growth_segments_divider\n",
    "    df_re_growth = preprocess_potential_users(df_re)\n",
    "    df_new_growth = preprocess_potential_users(df_new)\n",
    "    \n",
    "    return df_re_segment, df_re_growth, df_new_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m2aleYPfISX",
    "outputId": "d12d27c9-d3ee-4ca3-d315-38cab1276ce6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/th/.local/share/virtualenvs/th-EMsiyxSG/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/th/.local/share/virtualenvs/th-EMsiyxSG/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/th/.local/share/virtualenvs/th-EMsiyxSG/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/th/.local/share/virtualenvs/th-EMsiyxSG/lib/python3.6/site-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 1.71 s, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphereId</th>\n",
       "      <th>act_label</th>\n",
       "      <th>purchase_label</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kqVrm1oVTLqORTbE0k7oNuFsYaRsg</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-C-FSJ_P64E9TNKyojYgO9uOJIKrP</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>LIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K27K6kTMvdDugDuLeU6VeyqpXxGN3</td>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>LIGHT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PrbR8uI4vUZA_dDGfUbHGcZtNU6XW</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fVIUoaZ6_rRiEUrV1iAAHG57VxqaD</td>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        sphereId act_label purchase_label segment\n",
       "0  kqVrm1oVTLqORTbE0k7oNuFsYaRsg         M              0    None\n",
       "1  -C-FSJ_P64E9TNKyojYgO9uOJIKrP         H              0   LIGHT\n",
       "2  K27K6kTMvdDugDuLeU6VeyqpXxGN3         H              0   LIGHT\n",
       "3  PrbR8uI4vUZA_dDGfUbHGcZtNU6XW         L              0    None\n",
       "4  fVIUoaZ6_rRiEUrV1iAAHG57VxqaD         L              0    None"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# run\n",
    "today = datetime(2020,5,31)\n",
    "input_path = '/home/heemok/tand/data_coke/coke_android'#'../in/coke/android'\n",
    "key_id = 'sphereId'\n",
    "dates = 28\n",
    "idx = [i for i in range(dates)]\n",
    "date_list = [(today - timedelta(days=x)).date() for x in range(dates)]\n",
    "df_re_segment, df_re_growth, df_new_growth = segment_run (today, input_path, dates,\n",
    "                             key_id, newb_period = 7, \n",
    "                             user_property = False, commerce = True, platform_total=False) \n",
    "df_re_segment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuAuXKfqfISY",
    "outputId": "4fd839e9-a7ee-47d7-9f49-ca978ece69e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphereId</th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "      <th>real_date</th>\n",
       "      <th>mapping</th>\n",
       "      <th>real_duration</th>\n",
       "      <th>duration_corr</th>\n",
       "      <th>min_engage_2w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-5W_QnqpE98C2HY8bNvirzpMRqjLQ</td>\n",
       "      <td>[2020-05-17, 2020-05-18, 2020-05-19, 2020-05-2...</td>\n",
       "      <td>[265.0, 2708.0, 618.0, 599.0, 260.0, 144.0, 13...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-17: 265.0, 2020-05-18: 2708.0, 2020-0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 265.0,...</td>\n",
       "      <td>0.219627</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-7JSpraoM8WOt22JrHJ0qpdq6B-61</td>\n",
       "      <td>[2020-05-25, 2020-05-26, 2020-05-28]</td>\n",
       "      <td>[676.0, 301.0, 37.0]</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-25: 676.0, 2020-05-26: 301.0, 2020-05...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-7wxzKPpymbIp7oSw2s7nFE-QZKtU</td>\n",
       "      <td>[2020-05-19, 2020-05-20, 2020-05-21, 2020-05-2...</td>\n",
       "      <td>[510.0, 1228.0, 647.0, 408.0, 269.0, 232.0, 23...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-19: 510.0, 2020-05-20: 1228.0, 2020-0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.485639</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-8800JAP9EaVW2c0PauP_JvEDtIhq</td>\n",
       "      <td>[2020-05-25, 2020-05-26, 2020-05-27, 2020-05-2...</td>\n",
       "      <td>[527.0, 431.0, 84.0, 189.0, 20.0]</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-25: 527.0, 2020-05-26: 431.0, 2020-05...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.370359</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-8O3O5xPHImZ-krYK35PvA__VNHS0</td>\n",
       "      <td>[2020-05-03, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>[341.0, 101.0, 224.0, 141.0, 122.0, 145.0, 640...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-03: 341.0, 2020-05-05: 101.0, 2020-05...</td>\n",
       "      <td>[0, 101.0, 224.0, 141.0, 0, 0, 0, 0, 0, 122.0,...</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          sphereId  \\\n",
       "70   -5W_QnqpE98C2HY8bNvirzpMRqjLQ   \n",
       "90   -7JSpraoM8WOt22JrHJ0qpdq6B-61   \n",
       "95   -7wxzKPpymbIp7oSw2s7nFE-QZKtU   \n",
       "98   -8800JAP9EaVW2c0PauP_JvEDtIhq   \n",
       "101  -8O3O5xPHImZ-krYK35PvA__VNHS0   \n",
       "\n",
       "                                                  date  \\\n",
       "70   [2020-05-17, 2020-05-18, 2020-05-19, 2020-05-2...   \n",
       "90                [2020-05-25, 2020-05-26, 2020-05-28]   \n",
       "95   [2020-05-19, 2020-05-20, 2020-05-21, 2020-05-2...   \n",
       "98   [2020-05-25, 2020-05-26, 2020-05-27, 2020-05-2...   \n",
       "101  [2020-05-03, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "\n",
       "                                              duration  \\\n",
       "70   [265.0, 2708.0, 618.0, 599.0, 260.0, 144.0, 13...   \n",
       "90                                [676.0, 301.0, 37.0]   \n",
       "95   [510.0, 1228.0, 647.0, 408.0, 269.0, 232.0, 23...   \n",
       "98                   [527.0, 431.0, 84.0, 189.0, 20.0]   \n",
       "101  [341.0, 101.0, 224.0, 141.0, 122.0, 145.0, 640...   \n",
       "\n",
       "                                             real_date  \\\n",
       "70   [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "90   [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "95   [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "98   [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "101  [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "\n",
       "                                               mapping  \\\n",
       "70   {2020-05-17: 265.0, 2020-05-18: 2708.0, 2020-0...   \n",
       "90   {2020-05-25: 676.0, 2020-05-26: 301.0, 2020-05...   \n",
       "95   {2020-05-19: 510.0, 2020-05-20: 1228.0, 2020-0...   \n",
       "98   {2020-05-25: 527.0, 2020-05-26: 431.0, 2020-05...   \n",
       "101  {2020-05-03: 341.0, 2020-05-05: 101.0, 2020-05...   \n",
       "\n",
       "                                         real_duration  duration_corr  \\\n",
       "70   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 265.0,...       0.219627   \n",
       "90   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.262067   \n",
       "95   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.485639   \n",
       "98   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0.370359   \n",
       "101  [0, 101.0, 224.0, 141.0, 0, 0, 0, 0, 0, 122.0,...       0.204000   \n",
       "\n",
       "     min_engage_2w  \n",
       "70              13  \n",
       "90               3  \n",
       "95              12  \n",
       "98               5  \n",
       "101             11  "
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphereId</th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "      <th>real_date</th>\n",
       "      <th>mapping</th>\n",
       "      <th>real_duration</th>\n",
       "      <th>duration_corr</th>\n",
       "      <th>min_engage_2w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>Aj4N1S2FulvsdNUbeYx2jXs9OkEPo</td>\n",
       "      <td>[2020-05-07, 2020-05-08, 2020-05-12, 2020-05-1...</td>\n",
       "      <td>[59.0, 8.0, 365.0, 704.0, 67.0, 447.0, 11.0, 1...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-07: 59.0, 2020-05-08: 8.0, 2020-05-12...</td>\n",
       "      <td>[0, 0, 0, 59.0, 8.0, 0, 0, 0, 365.0, 704.0, 0,...</td>\n",
       "      <td>0.278139</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>fGm38ai9HVrnreqTNI5R9N5_uelWF</td>\n",
       "      <td>[2020-05-04, 2020-05-06, 2020-05-24, 2020-05-2...</td>\n",
       "      <td>[290.0, 70.0, 158.0, 290.0, 361.0, 581.0, 2067...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-04: 290.0, 2020-05-06: 70.0, 2020-05-...</td>\n",
       "      <td>[290.0, 0, 70.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0.526388</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>vLwKEUGzLNAqs4yi35MUtVKn91oLn</td>\n",
       "      <td>[2020-05-08, 2020-05-13, 2020-05-21, 2020-05-2...</td>\n",
       "      <td>[1.0, 355.0, 63.0, 56.0, 101.0, 225.0, 83.0, 2...</td>\n",
       "      <td>[2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...</td>\n",
       "      <td>{2020-05-08: 1.0, 2020-05-13: 355.0, 2020-05-2...</td>\n",
       "      <td>[0, 0, 0, 0, 1.0, 0, 0, 0, 0, 355.0, 0, 0, 0, ...</td>\n",
       "      <td>0.438966</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sphereId  \\\n",
       "1079  Aj4N1S2FulvsdNUbeYx2jXs9OkEPo   \n",
       "4029  fGm38ai9HVrnreqTNI5R9N5_uelWF   \n",
       "5457  vLwKEUGzLNAqs4yi35MUtVKn91oLn   \n",
       "\n",
       "                                                   date  \\\n",
       "1079  [2020-05-07, 2020-05-08, 2020-05-12, 2020-05-1...   \n",
       "4029  [2020-05-04, 2020-05-06, 2020-05-24, 2020-05-2...   \n",
       "5457  [2020-05-08, 2020-05-13, 2020-05-21, 2020-05-2...   \n",
       "\n",
       "                                               duration  \\\n",
       "1079  [59.0, 8.0, 365.0, 704.0, 67.0, 447.0, 11.0, 1...   \n",
       "4029  [290.0, 70.0, 158.0, 290.0, 361.0, 581.0, 2067...   \n",
       "5457  [1.0, 355.0, 63.0, 56.0, 101.0, 225.0, 83.0, 2...   \n",
       "\n",
       "                                              real_date  \\\n",
       "1079  [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "4029  [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "5457  [2020-05-04, 2020-05-05, 2020-05-06, 2020-05-0...   \n",
       "\n",
       "                                                mapping  \\\n",
       "1079  {2020-05-07: 59.0, 2020-05-08: 8.0, 2020-05-12...   \n",
       "4029  {2020-05-04: 290.0, 2020-05-06: 70.0, 2020-05-...   \n",
       "5457  {2020-05-08: 1.0, 2020-05-13: 355.0, 2020-05-2...   \n",
       "\n",
       "                                          real_duration  duration_corr  \\\n",
       "1079  [0, 0, 0, 59.0, 8.0, 0, 0, 0, 365.0, 704.0, 0,...       0.278139   \n",
       "4029  [290.0, 0, 70.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       0.526388   \n",
       "5457  [0, 0, 0, 0, 1.0, 0, 0, 0, 0, 355.0, 0, 0, 0, ...       0.438966   \n",
       "\n",
       "      min_engage_2w  \n",
       "1079              7  \n",
       "4029              7  \n",
       "5457              7  "
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re_growth.head()\n",
    "df_new_growth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1Neh1jYfISY"
   },
   "outputs": [],
   "source": [
    "df = read_files(today, input_path, dates, False)\n",
    "df = basic_preprocess(df)\n",
    "df_new, df_re = divide_visitor_v2(today, dates, df,key_id,\n",
    "                              newb_period=7,\n",
    "                              user_property = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpHxML-LfISY"
   },
   "source": [
    "# 신규성장 혹은 잠재 고객의 성장 기울기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64FMslpJfISZ"
   },
   "outputs": [],
   "source": [
    "define_df = df_new_growth # 신규 성장 고객\n",
    "#define_df = df_re_growth # 잠재 고객"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5irFa2kffISZ"
   },
   "outputs": [],
   "source": [
    "df_new_growth.shape, df_re_growth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scgoyVVFfISZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as sp\n",
    "from sklearn import datasets, linear_model\n",
    "def linear_reg(x, y):\n",
    "    slope, intercept, r_value, p_value, std_err =sp.linregress(x,y)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(np.array(x).reshape(1,-1), np.array(y).reshape(1,-1))\n",
    "    coef = regr.coef_\n",
    "    xf = np.linspace(min(x),max(x),100)\n",
    "    xf1 = xf.copy()\n",
    "    yf = (slope*xf)+intercept\n",
    "\n",
    "    return xf1, yf, slope, intercept, r_value, p_value, std_err, coef\n",
    "\n",
    "for index, row in define_df.iterrows():\n",
    "    print('-------------------------')\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.suptitle('growth plot')\n",
    "    ax1.plot(idx, row['real_duration'])\n",
    "    ax1.set_title('duration')\n",
    "    ax1.set(xlabel='date')\n",
    "\n",
    "    \n",
    "    pred_x, pred_y, slope, intercept, r_value, p_value, std_err, coef = linear_reg(idx,row['real_duration'])\n",
    "\n",
    "    ax3.plot(pred_x, pred_y,label='Linear fit', lw=3)\n",
    "    ax3.plot(idx, row['real_duration'])\n",
    "    #print('coef = ', coef)\n",
    "    print('slope = ', slope, '\\n', 'intercept = ', intercept)\n",
    "    print('r = ', r_value, '\\n', 'p = ', p_value)\n",
    "\n",
    "    #plt.plot(idx, row['real_click'], 'g') # plotting t, c separately \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s10WT9pCfISZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MuoX1xrfISZ"
   },
   "source": [
    "## 신규 성장성 검토 테스트 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qje23WuRfISZ"
   },
   "outputs": [],
   "source": [
    "# 0) besic setting\n",
    "today = datetime(2020,5,31)\n",
    "input_path = '../in/coke/android'\n",
    "key_id = 'sphereId'\n",
    "dates = 28\n",
    "\n",
    "today, input_path, dates\n",
    "key_id\n",
    "newb_period = 7\n",
    "user_property = False\n",
    "commerce = True\n",
    "platform_total = True # for import android and ios at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbHGCzy9fISa"
   },
   "outputs": [],
   "source": [
    "# # 1) data import\n",
    "# df = read_files(today, input_path, dates, platform_total = True)\n",
    "\n",
    "# # 2) Basic preprocess\n",
    "# df = basic_preprocess(df) # duration outlier\n",
    "\n",
    "# 3) divide_target\n",
    "df_new, df_re = divide_visitor_v2(today, dates, df,key_id,\n",
    "                                  newb_period=7,\n",
    "                                  user_property = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-MRRXA_fISa"
   },
   "outputs": [],
   "source": [
    "# check\n",
    "df_new.date.min()\n",
    "df_new.date.max() # 신규 고려 기간 4주(가입 첫주 포함)\n",
    "df_new[key_id].nunique() # 첫주에 가입한 회원수\n",
    "df_re[key_id].nunique()  # 4주간 모든 신규를 제외한(appInstall X) 나머지 기존 유저수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FySFV6QffISa"
   },
   "outputs": [],
   "source": [
    "df_new.head()\n",
    "len(df_re)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "segment_final_v3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
