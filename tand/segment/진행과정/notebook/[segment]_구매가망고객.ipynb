{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTNnSdznpu56"
   },
   "source": [
    "# 구매 가망 고객 추출\n",
    "    1. 목적 : 미래에 구매할 고객을 추출하여, 구매 행동이 존재하는 고객사들에 한하여 해당 고객에게 마케팅 액션을 취하여 KPI 증대에 기여\n",
    "    2. 구성 : 전체적인 구성은 기존의 이탈 분석을 벤치마킹하여 구현\n",
    "        2-1. 데이터 불러오기\n",
    "            - 구매 행동(ex) event : SapBuy-)이 존재하는 고객사(appkey)에 대하여 진행 가능\n",
    "            - X에 해당하는 독립변수의 기간은 28일, Y에 해당하는 종속변수의 기간은 7일로 설정\n",
    "            - 학습 데이터와 예측 데이터는 7일의 기간 차이를 나타냄\n",
    "\n",
    "        2-1. 데이터 전처리\n",
    "            - basic_preprocess 함수 활용 : duration 전처리 \n",
    "            - set_date_range 함수 활용 : 학습/예측 데이터 분리, label 생성\n",
    "            - feature_engineering_pipe 함수 활용 : aggregation을 이용하여 최종 데이터셋 생성\n",
    "          \n",
    "        2-2. 데이터 샘플링\n",
    "            - sampling_dataset 함수 활용 : 학습 데이터의 언더/오버 샘플링 진행\n",
    "            \n",
    "        2-3. 모델 학습\n",
    "            - xgboost 모델 활용\n",
    "            - 파라미터 튜닝은 미진행\n",
    "\n",
    "        2-3. 모델 결과\n",
    "            - test 데이터에 대한 예측 결과와 실제 결과 비교\n",
    "\n",
    "    3. 결과 : \n",
    "        3-1. 자세한 검증 결과는 매출 예측 프로젝트 ppt에 존재함.\n",
    "        3-2. 타 머신러닝 모델과의 성능 차이는 크게 나지 않지만, 해당 모델이 가장 성능이 높았음.\n",
    "        \n",
    "    4. 이슈 :\n",
    "        4-1. 구매 관련한 변수를 추가할 수 있으나, 해당 기간(28일) 내에 구매하지 않은 고객들이 많아서 28일 설정에 대한 고민을 해보아야 함.\n",
    "        4-2. label의 기간 설정이 7일로 되어 있어 그 이상으로 늘리는 시도를 해보아야 함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-TzMDocpu6B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(\"/home/das_share/common_class/\")\n",
    "sys.path.append(\"/home/das_share/analysis/\")\n",
    "import DataImportClass\n",
    "from DataImportClass import DataImport\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZBZYzaHpu6C"
   },
   "source": [
    "### 활용한 사용자정의 함수 리스트\n",
    "    1. read_files(데이터 불러오기 함수) - 기존 이탈 분석 함수와 동일\n",
    "    2. feature_engineering_pipe(변수 생성 함수) - 기존 이탈 분석 함수에서 변형\n",
    "        - event 변수 생성 : 기간 내 평균 클릭 횟수\n",
    "        - duration 변수 생성 : 기간 내 평균/최소/최대/표준편차 클릭 횟수\n",
    "        - 접속 변수 생성 : 기존 이탈 분석에 사용된 변수들과 동일\n",
    "        - 구매 접속 변수 생성 : 지난 7일 내 구매 횟수, 지난 4주 내 구매한 주\n",
    "    3. make_events - events 컬럼에서 event name을 가져오는 함수\n",
    "        - event 변수 생성 시 활용됨\n",
    "    4. get_event_click - event의 횟수를 세는 함수\n",
    "        - event 변수 생성 시 활용됨\n",
    "    5. get_date_of_purchase - events 컬럼에서 event name 중 구매 이벤트의 날짜를 가져오는 함수\n",
    "        - 구매 접속 변수 생성 시 활용됨\n",
    "    6. get_sess_date - 지난 7일 내 접속한 날짜를 구하기 위한 함수\n",
    "        - 접속 변수 생성 시 활용됨\n",
    "    7. calculate_contis - 기존 이탈 분석 함수와 동일\n",
    "        - 접속 변수 생성 시 활용됨\n",
    "    8. sampling_dataset - 기존 이탈 분석 함수와 동일\n",
    "        - 훈련 데이터의 오버샘플링 시 활용됨         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNHzWXTepu6C",
    "outputId": "37811950-b1be-431a-c978-8ad46a0c2d15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def read_files(today, input_path, dates, platform_total = False):\n",
    "    input_path_list = [input_path]\n",
    "    if platform_total == True :\n",
    "        input_path_list = []\n",
    "        input_path_list.append(input_path.split('/')[0] +'/'+input_path.split('/')[1]+'/'+input_path.split('/')[2]+'/android')\n",
    "        input_path_list.append(input_path.split('/')[0] +'/'+input_path.split('/')[1]+'/'+input_path.split('/')[2]+'/ios')  \n",
    "    df_list = []\n",
    "    for input_path in input_path_list :\n",
    "        for i in range(1, dates+1):\n",
    "            try : \n",
    "                date = today - timedelta(days=i) # 설정한 today를 기준으로 과거 n일 json\n",
    "                y = str(date.year)[2:]\n",
    "                m = str(date.month).zfill(2)\n",
    "                d = str(date.day).zfill(2)\n",
    "                filename = y+m+d+'.json'\n",
    "                df_tmp = pd.read_json(input_path+'/'+filename)\n",
    "                df_list.append(df_tmp)\n",
    "            except ValueError :\n",
    "                print(\"Not enough data to load.\")\n",
    "    df = pd.concat(df_list).reset_index(drop = True) # data merge\n",
    "    return df\n",
    "\n",
    "# 입력 데이터에 대하여 aggregation을 통한 변수를 만들어내는 함수\n",
    "# 접속변수는 이탈분석에서 사용된 방법과 동일\n",
    "def feature_engineering_pipe(df_tmp, churn, date1, date2, today):\n",
    "    # aggregate event\n",
    "    # sphereId 별 이벤트 클릭 수 저장\n",
    "    df_tmp['events_name_list'] = df_tmp.events.apply(lambda x: make_events(x))\n",
    "    df_event_gp = df_tmp.groupby('sphereId')['events_name_list'].apply(lambda x: list(itertools.chain(*x)))\n",
    "    lst_event_unique = list(set(itertools.chain(*df_event_gp.values)))\n",
    "    \n",
    "    lst_events_gp = []\n",
    "    for event in lst_event_unique:\n",
    "        lst_events_gp.append(df_event_gp.apply(lambda x: get_event_click(x, event)).values)\n",
    "\n",
    "    # aggregate duration\n",
    "    # sphereId 별 접속시간 [평균, 합계, 표준편차, 최소값, 최대값] 저장\n",
    "    lst_event_unique.append('duration__mean')\n",
    "    lst_events_gp.append(df_tmp.groupby('sphereId')['duration'].mean().values)\n",
    "    lst_event_unique.append('duration__sum')\n",
    "    lst_events_gp.append(df_tmp.groupby('sphereId')['duration'].sum().values)   \n",
    "    lst_event_unique.append('duration__std')\n",
    "    lst_events_gp.append(df_tmp.groupby('sphereId')['duration'].std().values)\n",
    "    lst_event_unique.append('duration__min')\n",
    "    lst_events_gp.append(df_tmp.groupby('sphereId')['duration'].min().values)\n",
    "    lst_event_unique.append('duration__max')\n",
    "    lst_events_gp.append(df_tmp.groupby('sphereId')['duration'].max().values)\n",
    "    # make data - event / duration\n",
    "    df_events_gp = pd.DataFrame(lst_events_gp).T\n",
    "    df_events_gp.columns = lst_event_unique\n",
    "    df_events_gp['sphereId'] = df_event_gp.index.values\n",
    "    df_events_gp = df_events_gp.fillna(0)\n",
    "    \n",
    "    # aggregate session\n",
    "    # 접속 변수 생성 - 이탈분석에 사용되는 방법과 동일\n",
    "    lst_sess_columns = ['sphereId','conti_act_days_last7','conti_act_weeks_last4','act_days_last7','act_weeks_last4',\n",
    "                     'conti_inact_days_last7','conti_inact_weeks_last4','days_since_first','days_since_last']\n",
    "    df_sess_gp = df_tmp.groupby('sphereId')['date'].apply(list).reset_index()\n",
    "    e = today.isocalendar()[1]\n",
    "    \n",
    "    df_sess_gp['last7'] = df_sess_gp['date'].apply(lambda x: get_sess_date(x, date2))\n",
    "    df_sess_gp['last4'] = df_sess_gp['date'].apply(lambda x: list(set(i.week for i in x)))\n",
    "    df_sess_gp['inact_days'] = df_sess_gp['last7'].apply(lambda x: list(set([y for y in range(date2.day, date2.day+7)]) - set(x)))\n",
    "    df_sess_gp['inact_weeks'] = df_sess_gp['last4'].apply(lambda x: list(set([y for y in range(e-3,e+1)]) - set(x)))\n",
    "    \n",
    "    df_sess_gp['conti_act_days_last7'] = df_sess_gp['last7'].apply(lambda x: calculate_contis(x))\n",
    "    df_sess_gp['conti_act_weeks_last4'] = df_sess_gp['last4'].apply(lambda x: calculate_contis(x))\n",
    "    df_sess_gp['conti_inact_days_last7'] = df_sess_gp['inact_days'].apply(lambda x: calculate_contis(x))\n",
    "    df_sess_gp['conti_inact_weeks_last4'] = df_sess_gp['inact_weeks'].apply(lambda x: calculate_contis(x))\n",
    "    df_sess_gp['act_days_last7'] = df_sess_gp['last7'].str.len()\n",
    "    df_sess_gp['act_weeks_last4'] = df_sess_gp['last4'].str.len()\n",
    "    df_sess_gp['days_since_first'] = df_sess_gp['date'].apply(lambda x: (today-x[0]).days)\n",
    "    df_sess_gp['days_since_last'] = df_sess_gp['date'].apply(lambda x: (today-x[-1]).days)\n",
    "    \n",
    "    # make data - session\n",
    "    df_sess_gp = df_sess_gp[lst_sess_columns]\n",
    "\n",
    "    # aggregate purchase session date\n",
    "    # 접속변수들과 동일하게 구매를 기준으로 하여 생성하려고 했으나, 대부분의 사람들이 구매를 하지 않기 때문에 오류가 발생하여\n",
    "    # 지난 7일간 구매 횟수, 지난 4주간 구매 횟수 변수만 추가하였음\n",
    "    df_tmp['date'] = pd.to_datetime(df_tmp['date'])\n",
    "    df_tmp['date_purchase'] = df_tmp[['date','events']].apply(lambda x: get_date_of_purchase(x), axis=1)\n",
    "    \n",
    "    df_sess_purchase_gp = df_tmp.groupby('sphereId')['date_purchase'].apply(lambda x: list(itertools.chain(*x))).reset_index()\n",
    "    df_sess_purchase_gp.columns = ['sphereId','date_purchase']\n",
    "    lst_sess_purchase_columns = [i+\"_purchase\" for i in lst_sess_columns if i != \"sphereId\"]\n",
    "    \n",
    "    df_sess_purchase_gp['last7'] = df_sess_purchase_gp['date_purchase'].apply(lambda x: get_sess_date(x, date2))\n",
    "    df_sess_purchase_gp['last4'] = df_sess_purchase_gp['date_purchase'].apply(lambda x: list(set(i.week for i in x)))\n",
    "\n",
    "    df_sess_purchase_gp['act_days_last7'] = df_sess_purchase_gp['last7'].str.len()\n",
    "    df_sess_purchase_gp['act_weeks_last4'] = df_sess_purchase_gp['last4'].str.len()\n",
    "\n",
    "    # make data - purchase session date\n",
    "    df_sess_purchase_gp = df_sess_purchase_gp[[\"sphereId\",\"act_days_last7\",\"act_weeks_last4\"]]\n",
    "    \n",
    "    # make label\n",
    "    # 구매 여부 변수 생성\n",
    "    df_sess_gp['churn'] = df_sess_gp['sphereId'].map(lambda x: 1 if x in churn else 0)\n",
    "\n",
    "    return df_events_gp, df_sess_gp, df_sess_purchase_gp\n",
    "\n",
    "# 각 event들의 횟수를 구하기 위해 사용하는 함수\n",
    "def make_events(x):\n",
    "    lst_events = []\n",
    "    for i in x:\n",
    "        lst_events.append(i['name'])\n",
    "    return lst_events\n",
    "\n",
    "# 기간 내에 모든 구매 날짜를 저장하기 위해 사용하는 함수\n",
    "def get_date_of_purchase(x):\n",
    "    lst_dates = []\n",
    "    if 'sapBuyStore' in str(x['events']):\n",
    "        lst_dates.append(x['date'])\n",
    "    return lst_dates\n",
    "\n",
    "# 총 event 횟수를 구하기 위해 사용하는 함수\n",
    "def get_event_click(x, event):\n",
    "    return x.count(event)\n",
    "\n",
    "# 지난 7일간 접속한 날짜를 구하기 위해 사용하는 함수\n",
    "def get_sess_date(lst_sess, flag_date):\n",
    "    result = []\n",
    "    for sess in lst_sess:\n",
    "        if sess > flag_date:\n",
    "            result.append(sess)\n",
    "        else:\n",
    "            pass\n",
    "        return list(set(result))\n",
    "\n",
    "def calculate_contis(lst): \n",
    "    output = 0\n",
    "    if len(lst) == 0:\n",
    "        return 0\n",
    "    elif len(lst) == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        conti = 1\n",
    "        for i in range(len(lst)-1):\n",
    "            if lst[i+1] - lst[i] == 1:\n",
    "                conti += 1\n",
    "                if conti > output:\n",
    "                    output = conti\n",
    "            else:\n",
    "                conti = 1\n",
    "        return output\n",
    "    \n",
    "# train 데이터를 샘플링하는 함수\n",
    "def sampling_dataset(X_train, y_train):\n",
    "    #smote = SMOTE(n_jobs=6, random_state=42)\n",
    "    #smote = RandomOverSampler(random_state=42)\n",
    "    undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "    cols = X_train.columns\n",
    "    X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "    X_train = pd.DataFrame(X_train, columns=cols)\n",
    "    X_train = X_train.fillna(0)\n",
    "    y_train = pd.Series(y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I81CguWkpu6H"
   },
   "source": [
    "### 데이터 불러오기 및 조건 설정\n",
    "    - 한번에 여러번의 분석 일자를 테스팅하기 위해, 기간을 길게 설정 후 데이터를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr3MDCIBpu6I"
   },
   "outputs": [],
   "source": [
    "# 계속 데이터를 불러오지 않고, 한번에 긴 데이터를 불러와서 저장하고 이를 사용했습니다.\n",
    "# 1) data import\n",
    "today = datetime(2020,5,31)\n",
    "input_path = '/home/heemok/tand/data_coke/coke_android'#'../in/coke/android'\n",
    "key_id = 'sphereId'\n",
    "dates = 50\n",
    "df = read_files(today, input_path, dates, platform_total)\n",
    "\n",
    "# 2) Basic preprocess\n",
    "df = basic_preprocess(df)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d %H:%M:%S').dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl9vuxUdpu6I"
   },
   "source": [
    "### 분석 실행\n",
    "    - for문을 활용하여, 앞서 불러온 전체 데이터를 분석일자에 맞게만 사용하면서 분석을 진행함\n",
    "    - 분석일자별로 리스트에 성능결과가 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "683lr3kNpu6I",
    "outputId": "b90b1cd5-d82a-436a-f6d2-65d0243d5eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heemok/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:31:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[13017  5404]\n",
      " [   39   330]]\n",
      "precision :  0.05755144750610394\n",
      "recall :  0.8943089430894309\n",
      "f1 :  0.1081435359659184\n",
      "auc :  0.8771766757893409\n",
      "--- 37.40783214569092s seconds---\n",
      "2021-01-30 00:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b8353a8a389f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtrain_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtest_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_window\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mdf_events_gp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sess_gp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sess_purchase_gp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheemok_pipe_hdc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_churn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mdf_events_gp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sess_gp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sess_purchase_gp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheemok_pipe_hdc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_churn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ea6d9fee16b3>\u001b[0m in \u001b[0;36mheemok_pipe_hdc\u001b[0;34m(df_tmp, churn, date1, date2, today)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mlst_events_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sphereId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# data - event / duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mdf_events_gp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_events_gp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdf_events_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst_event_unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mdf_events_gp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sphereId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_event_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    519\u001b[0m                             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                     \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mform_blocks\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mblock_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m         \u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# Need this first(ish) so that Sparse[datetime] is sparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtensionBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2696\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2697\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/heemok-SnMGMROh/lib/python3.6/site-packages/pandas/core/dtypes/base.py\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = []\n",
    "# 날짜별 분석 실행하여 분석 결과를 저장\n",
    "for today in [datetime(2021,1,31) - timedelta(days=i) for i in range(30)]:\n",
    "    print(today)\n",
    "    start_time = time.time()\n",
    "\n",
    "    date1 = today - timedelta(days=35)\n",
    "    date2 = today - timedelta(days=28)\n",
    "    date3 = today - timedelta(days=14)\n",
    "    date4 = today - timedelta(days=7)\n",
    "    date5 = today + timedelta(days=7)\n",
    "\n",
    "    # Set Windows\n",
    "    train_window = DataImport.set_date_range(date1, date4, df)\n",
    "    train_label = DataImport.set_date_range(date4, today, df)\n",
    "\n",
    "    test_window = DataImport.set_date_range(date2, today, df)\n",
    "    test_label = DataImport.set_date_range(today, date5, df)\n",
    "\n",
    "    # create purchase_label\n",
    "    label_df = train_label.groupby('sphereId')['events'].apply(list).reset_index()\n",
    "    label_df['events'] = label_df.events.astype('str').apply(lambda x: 1 if 'sapBuyStore' in x else 0)\n",
    "    label_df_test = test_label.groupby('sphereId')['events'].apply(list).reset_index()\n",
    "    label_df_test['events'] = label_df_test.events.astype('str').apply(lambda x: 1 if 'sapBuyStore' in x else 0)\n",
    "\n",
    "    # set label\n",
    "    train_ids = set(train_window['sphereId'])\n",
    "    test_ids = set(test_window['sphereId'])\n",
    "\n",
    "    train_label_ids = set(label_df.loc[label_df['events']==1].sphereId)\n",
    "    train_churn = train_label_ids#train_ids - train_label_ids\n",
    "    test_label_ids = set(label_df_test.loc[label_df_test['events']==1].sphereId)\n",
    "    test_churn = test_label_ids#test_ids - test_label_ids\n",
    "    \n",
    "    train_window['date'] = pd.to_datetime(train_window['date'])\n",
    "    test_window['date'] = pd.to_datetime(test_window['date'])\n",
    "    \n",
    "    # feature engineering\n",
    "    df_events_gp_train, df_sess_gp_train, df_sess_purchase_gp_train = feature_engineering_pipe(train_window, train_churn, date1, date3, date4)\n",
    "    df_events_gp_test, df_sess_gp_test, df_sess_purchase_gp_test = feature_engineering_pipe(test_window, test_churn, date2, date4, today)\n",
    "    \n",
    "    # merge data\n",
    "    df_train = pd.merge(df_events_gp_train, df_sess_gp_train, on='sphereId', how='outer')\n",
    "    df_train = pd.merge(df_train, df_sess_purchase_gp_train, on='sphereId', how='outer')\n",
    "    df_test = pd.merge(df_events_gp_test, df_sess_gp_test, on='sphereId', how='outer')\n",
    "    df_test = pd.merge(df_test, df_sess_purchase_gp_test, on='sphereId', how='outer')\n",
    "    \n",
    "    # prepare data\n",
    "    X_train, X_test = df_train.drop(['sphereId','churn'], axis=1), df_test.drop(['sphereId','churn'], axis=1)\n",
    "    y_train, y_test = df_train['churn'], df_test['churn']\n",
    "    \n",
    "    # sampling train data\n",
    "    X_train, y_train = sampling_dataset(X_train, y_train)\n",
    "\n",
    "    feat = list(set(X_test.columns)&set(X_train.columns))\n",
    "    X_train = X_train[feat]\n",
    "    X_test = X_test[feat].fillna(0)\n",
    "    \n",
    "    # modelling\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train.values,y_train.values)\n",
    "\n",
    "    y_pred = model.predict(X_test.values)\n",
    "    y_pred_proba = model.predict_proba(X_test.values)\n",
    "    \n",
    "    # evaluation\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    result.append([precision,recall,f1,auc])\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('precision : ',precision)\n",
    "    print('recall : ', recall)\n",
    "    print('f1 : ', f1)\n",
    "    print('auc : ', auc)\n",
    "    print(\"--- {}s seconds---\".format(time.time()-start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[segment]_구매가망고객.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
